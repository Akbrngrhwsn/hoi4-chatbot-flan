import os
import torch
import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain.chains import RetrievalQA

# ==========================
# Konfigurasi Streamlit
# ==========================
st.set_page_config(page_title="HOI4 Chatbot (FLAN-T5)", layout="wide")
st.title("üß† HOI4 Chatbot (FLAN-T5 Local)")
st.markdown("Tanya jawab berbasis Wiki Hearts of Iron IV menggunakan model **FLAN-T5** secara lokal.")

# ==========================
# Load Vectorstore
# ==========================
st.info("üì• Memuat vectorstore...")

embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore_path = "hoi4_vectorstore"

if not os.path.exists(vectorstore_path):
    st.error("‚ùå Folder vectorstore tidak ditemukan. Jalankan proses penyimpanan terlebih dahulu.")
    st.stop()

@st.cache_resource
def load_vectorstore(path, _embeddings):
    return FAISS.load_local(path, _embeddings, allow_dangerous_deserialization=True)

vectorstore = load_vectorstore(vectorstore_path, embedding_model)
st.success("‚úÖ Vectorstore berhasil dimuat.")

# ==========================
# Load Model FLAN-T5 Lokal
# ==========================
st.info("ü§ñ Memuat model FLAN-T5...")

@st.cache_resource
def load_flan_model():
    tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
    model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")

    if torch.cuda.is_available():
        model = model.to("cuda")

    flan_pipeline = pipeline(
        "text2text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.7
    )
    return HuggingFacePipeline(pipeline=flan_pipeline)

flan_llm = load_flan_model()
st.success("‚úÖ Model FLAN-T5 berhasil dimuat.")

# ==========================
# Buat Chain QnA
# ==========================
flan_qa = RetrievalQA.from_chain_type(llm=flan_llm, retriever=vectorstore.as_retriever())

# ==========================
# Session State Chat
# ==========================
if "flan_messages" not in st.session_state:
    st.session_state.flan_messages = []

# ==========================
# UI Input
# ==========================
st.markdown("---")
st.subheader("üí¨ Tanyakan sesuatu seputar HoI4")
user_input = st.text_input("Pertanyaan:", placeholder="Contoh: Bagaimana cara membuat faksi sendiri?", key="flan_user_input")

# ==========================
# Proses Jawaban
# ==========================
if user_input:
    with st.spinner("üîç Mengambil jawaban dari FLAN-T5..."):
        try:
            result = flan_qa.invoke({"query": user_input})
            answer = result["result"] if isinstance(result, dict) else result
            st.session_state.flan_messages.append((user_input, answer))
        except Exception as e:
            st.session_state.flan_messages.append((user_input, f"‚ùå Error: {e}"))

# ==========================
# Tampilkan Riwayat
# ==========================
st.markdown("---")
st.subheader("üìú Riwayat Chat")

if st.session_state.flan_messages:
    for idx, (q, a) in enumerate(reversed(st.session_state.flan_messages), 1):
        st.markdown(f"### üîπ Pertanyaan {idx}:")
        st.markdown(f"**üßç Kamu:** {q}")
        st.markdown("üß† **FLAN-T5:**")
        st.success(a)
        st.markdown("---")
else:
    st.info("Belum ada percakapan.")
